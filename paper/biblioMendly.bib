@article{boulesteix2007partial,
  author       = {Boulesteix, Anne-laure and Strimmer, Korbinian},
  doi          = {10.1093/bib/bbl016},
  journal      = {Briefings in bioinformatics},
  keywords     = {classification,dimension reduction,gene
                  expression,high-dimensional genomic data,partial
                  least squares,pls},
  number       = 1,
  pages        = {32--44},
  publisher    = {Oxford Univ Press},
  title        = {{Partial least squares : a versatile tool for the
                  analysis of high-dimensional genomic data}},
  volume       = 8,
  year         = 2006
}

@article{Chun_2010,
  abstract     = {Partial least squares regression has been an
                  alternative to ordinary least squares for handling
                  multicollinearity in several areas of scientific
                  research since the 1960s. It has recently gained
                  much attention in the analysis of high dimensional
                  genomic data. We show that known asymptotic
                  consistency of the partial least squares estimator
                  for a univariate response does not hold with the
                  very large p and small n paradigm. We derive a
                  similar result for a multivariate response
                  regression with partial least squares. We then
                  propose a sparse partial least squares formulation
                  which aims simultaneously to achieve good predictive
                  performance and variable selection by producing
                  sparse linear combinations of the original
                  predictors. We provide an efficient implementation
                  of sparse partial least squares regression and
                  compare it with well-known variable selection and
                  dimension reduction approaches via simulation
                  experiments. We illustrate the practical utility of
                  sparse partial least squares regression in a joint
                  analysis of gene expression and genomewide binding
                  data.},
  author       = {Chun, Hyonho and Kele≈ü, S{\"{u}}nd{\"{u}}z},
  doi          = {10.1111/j.1467-9868.2009.00723.x},
  isbn         = {1369-7412 (Print)$\backslash$r1369-7412 (Linking)},
  issn         = 13697412,
  journal      = {Journal of the Royal Statistical Society. Series B:
                  Statistical Methodology},
  keywords     = {Chromatin immuno-precipitation,Dimension
                  reduction,Gene expression,Lasso,Microarrays,Partial
                  least squares,Sparsity,Variable and feature
                  selection},
  month        = {jan},
  number       = 1,
  pages        = {3--25},
  pmid         = 20107611,
  publisher    = {Wiley-Blackwell},
  title        = {{Sparse partial least squares regression for
                  simultaneous dimension reduction and variable
                  selection}},
  url          = {http://dx.doi.org/10.1111/j.1467-9868.2009.00723.x},
  volume       = 72,
  year         = 2010
}

@article{cook2013envelopes,
  abstract     = {We build connections between envelopes , a recently
                  proposed context for efficient estima-5 tion in
                  multivariate statistics, and multivariate partial
                  least squares (PLS) regression . In partic- 6 ular,
                  we establish an envelope as the nucleus of both
                  univariate and multivariate PLS, ... $\backslash$n},
  author       = {Cook, R D and Helland, I S and Su, Z},
  journal      = {Preprint},
  keywords     = {dimension reduction,envelope
                  models,envelopes,estimation,maximum
                  likelihood,partial least squares,simpls algorithm},
  number       = 5,
  pages        = {851--877},
  publisher    = {Wiley Online Library},
  title        = {{Envelopes and partial least squares regression}},
  url          =
                  {http://users.stat.umn.edu/{~}rdcook/RecentArticles/envPLSrev12.12.12.pdf{\%}5Cnpapers2://publication/uuid/ECDABE65-B382-4140-BE26-CD12A95E97D0},
  volume       = 75,
  year         = 2012
}

@article{Cook_2016fastEnv,
  abstract     = {Abstract We propose a new algorithm for envelope
                  estimation, along with a new n -consistent method
                  for computing starting values. The new algorithm,
                  which does not require optimization over a
                  Grassmannian, is shown by simulation to be much
                  faster and typically more accurate than the best
                  existing algorithm proposed by Cook and Zhang
                  (2016). },
  author       = {Cook, R Dennis and Forzani, Liliana and Su, Zhihua},
  doi          = {http://dx.doi.org/10.1016/j.jmva.2016.05.006},
  issn         = {0047-259X},
  journal      = {Journal of Multivariate Analysis},
  keywords     = {Envelopes,Grassmann manifold,Reducing subspaces},
  month        = {sep},
  pages        = {42--54},
  publisher    = {Elsevier BV},
  title        = {{A note on fast envelope estimation}},
  url          =
                  {http://www.sciencedirect.com/science/article/pii/S0047259X16300276},
  volume       = 150,
  year         = 2016
}

@article{cook2010envelope,
  abstract     = {We propose a new parsimonious version of the
                  classical multivariate normal linear model, yielding
                  a maximum likelihood estimator (MLE) that is
                  asymptoti- cally less variable than the MLE based on
                  the usual model. Our approach is based on the
                  construction of a link between the mean function and
                  the covariance ma- trix, using the minimal reducing
                  subspace of the latter that accommodates the former.
                  This leads to a multivariate regression model, which
                  we call the envelope model, where the number of
                  parameters is maximally reduced. The MLE from the
                  envelope model can be substantially less variable
                  than the usual MLE, especially when the mean
                  function varies in directions that are orthogonal to
                  the directions of maximum variation for the
                  covariance matrix.},
  author       = {Cook, R Dennis and Li, Bing and Chiaromonte,
                  Francesca},
  isbn         = {1017-0405},
  issn         = 10170405,
  journal      = {Statistica Sinica},
  keywords     = {discriminant analysis,functional data
                  analysis,grassmann,ing subspaces,invariant
                  subspaces,manifolds,phrases,principal
                  components,reduc,reduced rank regression,sufficient
                  dimension reduction},
  number       = 3,
  pages        = {927--1010},
  publisher    = {JSTOR},
  title        = {{Envelope Models for Parsimonious and Efficient
                  Multivariate Linear Regression}},
  url          =
                  {http://www.stat.berkeley.edu/{~}garveshr/Papers/SinicaEnvelopeModels.pdf},
  volume       = 20,
  year         = 2010
}

@article{cook2015envlp,
  author       = {Cook, R. Dennis and Su, Zhihua and Yang, Yi},
  doi          = {http://dx.doi.org/10.18637/jss.v062.i08},
  issn         = {1548-7660},
  journal      = {Journal of Statistical Software},
  number       = 8,
  pages        = {??--??},
  publisher    = {Foundation for Open Access Statistics},
  title        = {{$\backslash$pkgenvlp: A MATLAB Toolbox for
                  Computing Envelope Estimators in Multivariate
                  Analysis}},
  url          = {http://www.jstatsoft.org/v62/i08},
  volume       = 62,
  year         = 2015
}

@article{Cook_2015,
  author       = {Cook, R. Dennis and Zhang, Xin},
  doi          = {10.1080/01621459.2014.983235},
  issn         = {0162-1459},
  journal      = {Journal of the American Statistical Association},
  keywords     = {generalized linear models,grassmannians,weighted
                  least squares},
  month        = {apr},
  number       = {December},
  pages        = {00--00},
  publisher    = {Informa UK Limited},
  title        = {{Foundations for Envelope Models and Methods}},
  url          =
                  {http://www.tandfonline.com/doi/abs/10.1080/01621459.2014.983235},
  volume       = 1459,
  year         = 2014
}

@article{Cook_2016EnvAlgorithm,
  abstract     = {Envelopes were recently proposed as methods for
                  reducing estimative variation in multivariate linear
                  regression. Estimation of an envelope usually
                  involves optimization over Grassmann manifolds. We
                  propose a fast and widely applicable one-dimensional
                  (1D) algorithm for estimating an envelope in
                  general. We reveal an important structural property
                  of envelopes that facilitates our algorithm, and we
                  prove both Fisher consistency and n-consistency of
                  the algorithm.},
  archivePrefix= {arXiv},
  arxivId      = {1403.4138},
  author       = {Cook, R. Dennis and Zhang, Xin},
  doi          = {10.1080/10618600.2015.1029577},
  eprint       = {1403.4138},
  issn         = {1061-8600},
  journal      = {Journal of Computational and Graphical Statistics},
  keywords     = {Envelopes,Grassmann manifold,reducing subspaces},
  month        = {jan},
  number       = {March},
  pages        = {00--00},
  publisher    = {Informa UK Limited},
  title        = {{Algorithms for Envelope Estimation}},
  url          =
                  {http://www-tandfonline-com.focus.lib.kth.se/doi/abs/10.1080/10618600.2015.1029577},
  volume       = 8600,
  year         = 2015
}

@article{cook2015simultaneous,
  abstract     = {We introduce envelopes for simultaneously reducing
                  the predictors and the responses in multivariate
                  linear regression, so the regression then depends
                  only on estimated linear combinations of X and Y. We
                  use a likelihood-based objective function for
                  estimating envelopes and then propose algorithms for
                  estimation of a simultaneous envelope as well as for
                  basic Grassmann manifold optimization. The
                  asymptotic properties of the resulting estimator are
                  studied under normality and extended to general
                  distributions. We also investigate likelihood ratio
                  tests and information criteria for determining the
                  simultaneous envelope dimensions. Simulation studies
                  and real data examples show substantial gain over
                  the classical methods, like partial least squares,
                  canonical correlation analysis, and reduced-rank
                  regression. This article has supplementary material
                  available online.},
  author       = {Cook, R. Dennis and Zhang, Xin},
  doi          = {10.1080/00401706.2013.872700},
  issn         = {0040-1706},
  journal      = {Technometrics},
  keywords     = {canonical correlations,envelope model,grassmann
                  manifold,partial least squares,principal component
                  analysis,reduced-rank regression,sufficient
                  dimension reduction},
  number       = 1,
  pages        = {11--25},
  publisher    = {Taylor {\&} Francis},
  title        = {{Simultaneous Envelopes for Multivariate Linear
                  Regression}},
  url          =
                  {http://www.tandfonline.com/doi/abs/10.1080/00401706.2013.872700},
  volume       = 57,
  year         = 2014
}

@article{Frank_1993,
  abstract     = {discussion},
  author       = {Frank, Ildiko E. and Friedman, Jerome H.},
  doi          = {10.2307/1269656},
  isbn         = {0040-1706},
  issn         = 00401706,
  journal      = {Technometrics},
  keywords     = {multiple},
  month        = {may},
  number       = 2,
  pages        = {109--135},
  publisher    = {Informa UK Limited},
  title        = {{A Statistical View of Some Chemometrics Regression
                  Tools}},
  url          =
                  {http://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485035{\%}5Cnhttp://www.jstor.org/stable/1269656?origin=crossref},
  volume       = 35,
  year         = 1993
}

@article{Garthwaite_1994,
  abstract     = {Partial Least Squares},
  author       = {Garthwaite, P. H.},
  doi          = {10.2307/2291207},
  isbn         = 01621459,
  issn         = 01621459,
  journal      = {Journal of the American Statistical Association},
  keywords     = {1990,and fearn in the,biased regression,data
                  reduction,discussion of,is a comparatively
                  new,method stone and brooks,partial least
                  squares,pls,prediction,regressor construction,should
                  one believe,the contributions of brown,then,why},
  month        = {mar},
  number       = 425,
  pages        = {122--127},
  pmid         = 9501130702,
  publisher    = {Informa UK Limited},
  title        = {{An interpretation of partial least squares}},
  url          = {http://oro.open.ac.uk/17959/},
  volume       = 89,
  year         = 1994
}

@misc{hastie2009elements,
  abstract     = {Data Mining, Inference, and Prediction, Second
                  Edition},
  author       = {Hastie, Trevor and Tibshirani, Robert and Friedman,
                  Jerome},
  doi          = {10.1007/978-0-387-84858-7},
  isbn         = {978-0-387-84857-0},
  pages        = {XXII, 745},
  publisher    = {New York: Springer},
  title        = {{The Elements of Statistical Learning (2nd
                  edition)}},
  year         = 2009
}

@article{Helland_2001,
  author       = {Helland, I S},
  doi          = {10.1090/conm/287/04783},
  isbn         = {http://id.crossref.org/isbn/9780821878774},
  issn         = {0271-4132},
  journal      = {Algebraic Methods in Statistics},
  pages        = {139--153},
  publisher    = {American Mathematical Society (AMS)},
  title        = {{Reduction of regression models under symmetry}},
  url          = {http://dx.doi.org/10.1090/conm/287/04783},
  volume       = 287,
  year         = 2001
}

@book{helland2010steps,
  abstract     = {Culture, in fact, also plays an important role in
                  science which is, per se, a multitude of different
                  cultures. The book attempts to build a bridge across
                  three cultures: mathematical statistics, quantum
                  theory and chemometrical methods. Of course, these
                  three domains should not be taken as equals in any
                  sense. But the book holds the important claim that
                  it is possible to develop a common language which,
                  at least to a certain extent, can create direct
                  links and build bridges. From this point of
                  departure, the book will be of interest to the
                  following three types of scientists ‚Äî statisticians,
                  quantum physicists and chemometricians ‚Äî and in
                  particular, statisticians and physicists who are
                  interested in interdisciplinary research. Written at
                  a level that is accessible to general readers, not
                  only the academics, the book will appeal to graduate
                  students and mathematically educated persons of all
                  disciplines as well as philosophers, pure and
                  applied mathematicians, and the general
                  public.$\backslash$n$\backslash$nContents:$\backslash$n$\backslash$n*
                  The Basic Elements$\backslash$n* Statistical Theory
                  and Practice$\backslash$n* Statistical Inference
                  Under Symmetry$\backslash$n* The Transition from
                  Statistics to Quantum Theory$\backslash$n* Quantum
                  Mechanics from a Statistical Basis$\backslash$n*
                  Further Development of Quantum
                  Mechanics$\backslash$n* Decisions in
                  Statistics$\backslash$n* Multivariate Data Analysis
                  and Statistics$\backslash$n* Quantum Mechanics and
                  the Diversity of
                  Concepts$\backslash$n$\backslash$n$\backslash$nReadership:
                  Graduate students and researchers in the field of
                  statistics and mathematical physics.},
  author       = {Helland, Inge S},
  booktitle    = {Methods},
  doi          = {10.1142/9789814280860},
  isbn         = 9789814280860,
  pages        = 274,
  publisher    = {World Scientific},
  title        = {{Steps Towards a Unified Basis for Scientific Models
                  and Methods}},
  year         = 2010
}

@article{helland2004statistical,
  abstract     = {We explore the consequences of adjoining a symmetry
                  group to a statistical model. Group actions are
                  first induced on the sample space, and then on the
                  parameter space. It is argued that the right
                  invariant measure induced by the group on the
                  parameter space is a natural non-informative prior
                  for the parameters of the model. The permissible
                  sub-parameters are introduced, i.e., the
                  subparameters upon which group actions can be
                  defined. Equivariant estimators are similarly
                  defined. Orbits of the group are defined on the
                  sample space and on the parameter space; in
                  particular the group action is called transitive
                  when there is only one orbit. Credibility sets and
                  confidence sets are shown (under right invariant
                  prior and assuming transitivity on the parameter
                  space) to be equal when defined by permissible
                  sub-parameters and constructed from equivariant
                  estimators. The effect of different choices of
                  transformation group is illustrated by examples, and
                  properties of the orbits on the sample space and on
                  the parameter space are discussed. It is argued that
                  model reduction should be constrained to one or
                  several orbits of the group. Using this and other
                  natural criteria and concepts, among them concepts
                  related to design of experiments under symmetry,
                  leads to links towards chemometrical prediction
                  methods and towards the foundation of quantum
                  theory. On suppose q'un groupe de transformations
                  s'ex{\'{e}}cute {\`{a}} l'espace de donn{\'{e}}es et
                  induit un groupe de transformations {\`{a}} l'espace
                  de param{\`{e}}tre. Une classe de fonctions
                  param{\`{e}}tric importante‚Äîles
                  sous-param{\`{e}}tres permissibles‚Äîest intruit.
                  R{\'{e}}gions de credibilites (avec mesure
                  invariante droite comme prior) et r{\'{e}}gions de
                  confidence sommes d{\'{e}}montr{\'{e}}es
                  d'{\^{e}}tre {\'{e}}gal quand g{\'{e}}ner{\'{e}}es
                  par des sous-param{\`{e}}tres permissibles. L'effet
                  du choix des groupes des transformations est
                  illustr{\'{e}}e par des exemples. On montre qu'il
                  exist des relations contre methodes de prediction
                  dans chemometrie, contre des design des
                  exp{\'{e}}riences et contre de physique quantique.},
  author       = {Helland, Inge S},
  doi          = {10.1111/j.1751-5823.2004.tb00245.x},
  issn         = 03067734,
  journal      = {International Statistical Review},
  keywords     = { gts{\_}statfound,gt{\_}cmc},
  number       = 3,
  pages        = {409--422},
  publisher    = {Wiley Online Library},
  title        = {{Statistical Inference under Symmetry}},
  url          =
                  {http://dx.doi.org/10.1111/j.1751-5823.2004.tb00245.x},
  volume       = 72,
  year         = 2004
}

@article{helland1994comparison,
  author       = {Helland, Inge S and Almoy, Trygve},
  doi          = {10.1080/01621459.1994.10476783},
  issn         = {0162-1459},
  journal      = {Journal of the American Statistical Association},
  keywords     = {Expected Prediction error,Partial Least Squares
                  Regression,Prediction Ability,Principal Component
                  Regression,relevant Components},
  number       = 426,
  pages        = {583--591},
  publisher    = {Taylor {\&} Francis Group},
  title        = {{Comparsion of Prediction Methods When Only a Few
                  Components are Relevant}},
  volume       = 89,
  year         = 1994
}

@article{Cook_2014plsTheory,
  abstract     = {We give a survey of partial least squares regression
                  with one y variable from a theoretical point of
                  view. Some general comments are made on the
                  motivation as seen by a statistician to study
                  particular chemometric methods, and the concept of
                  soft modelling is criticized from the same angle.
                  Various aspects of the PLS algorithm are considered
                  and the population PLS model is defined. Asymptotic
                  properties of the prediction error are briefly
                  discussed and the relation to other regression
                  methods are commented upon. Results indicating
                  positive and negative properties of PLSR are
                  mentioned, in particular the recent result of
                  Butler, Denham and others which seem to show that
                  PLSR can not be an optimal regression method in any
                  reasonable way. The only possible path left towards
                  some kind of optimality, it seems, is by first
                  trying to find a good motivation for the population
                  model and then possibly finding an optimal estimator
                  under this model. Some results on this are sketched.
                  {\textcopyright} 2001 Elsevier Science B.V. All
                  rights reserved.},
  author       = {Helland, Inge S.},
  doi          = {10.1016/S0169-7439(01)00154-X},
  isbn         = {0169-7439},
  issn         = 01697439,
  journal      = {Chemometrics and Intelligent Laboratory Systems},
  keywords     = {Biased regression methods,Continuum
                  regression,PCR,PLS,PLS algorithm,PLSR,Population
                  model,Prediction,Prediction
                  error,Regression,Relevant components,Ridge
                  regression,Shrinkage},
  month        = {jan},
  number       = 2,
  pages        = {97--107},
  publisher    = {Wiley-Blackwell},
  title        = {{Some theoretical aspects of partial least squares
                  regression}},
  url          = {http://dx.doi.org/10.1111/1467-9469.00144
                  http://dx.doi.org/10.1111/1467-9469.00085
                  http://dx.doi.org/10.1016/j.chemolab.2015.05.012
                  http://dx.doi.org/10.1111/1467-9868.00262
                  http://dx.doi.org/10.1016/0039-9140(92)80060-q
                  http://dx.doi.org/10.1002/cem.276},
  volume       = 58,
  year         = 2001
}

@article{Helland_2000,
  author       = {Helland, Inge S.},
  doi          = {10.1111/1467-9469.00174},
  issn         = {0303-6898},
  journal      = {Scandinavian Journal of Statistics},
  keywords     = {cation,classi,invariant space,mean squared
                  prediction error,model reduction,partial least
                  squares regression,prediction,random x,regression
                  analysis},
  month        = {mar},
  number       = 1,
  pages        = {1--20},
  publisher    = {Wiley-Blackwell},
  title        = {{Model Reduction for Prediction in Regression
                  Models}},
  url          = {http://doi.wiley.com/10.1111/1467-9469.00174},
  volume       = 27,
  year         = 2000
}

@article{Helland_2001,
  abstract     = {We give a survey of partial least squares regression
                  with one y variable from a theoretical point of
                  view. Some general comments are made on the
                  motivation as seen by a statistician to study
                  particular chemometric methods, and the concept of
                  soft modelling is criticized from the same angle.
                  Various aspects of the PLS algorithm are considered
                  and the population PLS model is defined. Asymptotic
                  properties of the prediction error are briefly
                  discussed and the relation to other regression
                  methods are commented upon. Results indicating
                  positive and negative properties of PLSR are
                  mentioned, in particular the recent result of
                  Butler, Denham and others which seem to show that
                  PLSR can not be an optimal regression method in any
                  reasonable way. The only possible path left towards
                  some kind of optimality, it seems, is by first
                  trying to find a good motivation for the population
                  model and then possibly finding an optimal estimator
                  under this model. Some results on this are sketched.
                  {\textcopyright} 2001 Elsevier Science B.V. All
                  rights reserved.},
  author       = {Helland, Inge S.},
  doi          = {10.1016/S0169-7439(01)00154-X},
  isbn         = {0169-7439},
  issn         = 01697439,
  journal      = {Chemometrics and Intelligent Laboratory Systems},
  keywords     = {Biased regression methods,Continuum
                  regression,PCR,PLS,PLS algorithm,PLSR,Population
                  model,Prediction,Prediction
                  error,Regression,Relevant components,Ridge
                  regression,Shrinkage},
  month        = {oct},
  number       = 2,
  pages        = {97--107},
  publisher    = {Elsevier BV},
  title        = {{Some theoretical aspects of partial least squares
                  regression}},
  url          = {http://dx.doi.org/10.1016/s0169-7439(01)00154-x},
  volume       = 58,
  year         = 2001
}

@article{Helland_1988,
  abstract     = {We prove that the two algorithms given in the
                  literature for partial least squares regression are
                  equivalent, and use this equivalence to give an
                  explicit formula for the resulting prediction
                  equation. This in turn is used to investigate the
                  regression method from several points of view. Its
                  relation to principal component regression is
                  clearified, and some heuristic arguments are given
                  to explain why partial least squares regression
                  often needs fewer factors to give its optimal
                  prediction.},
  author       = {Helland, Inge S.},
  doi          = {10.1080/03610918808812681},
  issn         = {0361-0918},
  journal      = {Communications in Statistics - Simulation and
                  Computation},
  keywords     = {Biased Regression,PLS-Algorithms,Partial Least
                  Squares Regression,Prediction Equation,Relevant
                  Factors},
  month        = {jan},
  number       = 2,
  pages        = {581--607},
  publisher    = {Informa UK Limited},
  title        = {{On the structure of partial least squares
                  regression}},
  url          =
                  {http://www.tandfonline.com/doi/abs/10.1080/03610918808812681},
  volume       = 17,
  year         = 1988
}

@article{helland1990partial,
  abstract     = {The calibration method PLS1 is described in terms of
                  the joint covariance structure of the explanatory
                  variables and the predicted variable. In the
                  population version it is possible to give simple
                  conditions for when the PLS algorithm stops after a
                  certain number of steps, and it turns out that the
                  resulting predictor is the same as the one given by
                  principal component regression. The concept of
                  relevant components is defined, and the relationship
                  to factor analysis models is discussed. Finally, the
                  implications for the sample version of PLS are
                  considered, both for the case when it is used as a
                  prediction method, and for the case when scores and
                  loadings from PLS--in a similar way as the scores
                  and loadings from factor analysis--are used in the
                  interpretation of data.},
  author       = {Helland, Inge S.},
  doi          = {10.2307/4616159},
  issn         = {0303-6898},
  journal      = {Scandinavian Journal of Statistics},
  number       = 2,
  pages        = {97--114},
  publisher    = {JSTOR},
  title        = {{Partial least squares regression and statistical
                  models}},
  url          = {http://www.jstor.org/stable/4616159},
  volume       = 17,
  year         = 1990
}

@article{helland_2012,
  abstract     = {Abstract.‚ÄÇThe random x regression model is
                  approached through the group of rotations of the
                  eigenvectors for the x-covariance matrix together
                  with scale transformations for each of the
                  corresponding regression coefficients. The partial
                  least squares model can be constructed from the
                  orbits of this group. A generalization of Pitman's
                  Theorem says that the best equivariant estimator
                  under a group is given by the Bayes estimator with
                  the group's invariant measure as the prior. A
                  straightforward application of this theorem turns
                  out to be impossible since the relevant invariant
                  prior leads to a non-defined posterior. Nevertheless
                  we can devise an approximate scale group with a
                  proper invariant prior leading to a well-defined
                  posterior distribution with a finite mean. This
                  Bayes estimator is explored using Markov chain Monte
                  Carlo technique. The estimator seems to require
                  heavy computations, but can be argued to have
                  several nice properties. It is also a valid
                  estimator when p{\textgreater}n.},
  author       = {Helland, Inge S. and Saeb??, Solve and Tjelmeland,
                  Ha Kon},
  doi          = {10.1111/j.1467-9469.2011.00770.x},
  issn         = 03036898,
  journal      = {Scandinavian Journal of Statistics},
  keywords     = {Bayesian estimation,Envelope
                  model,Equivariance,Group,Invariant measure,Markov
                  chain Monte Carlo,Partial least squares
                  model,Prediction,Relevant components},
  month        = {mar},
  number       = 4,
  pages        = {695--713},
  publisher    = {Wiley-Blackwell},
  title        = {{Near Optimal Prediction from Relevant Components}},
  url          = {http://dx.doi.org/10.1111/j.1467-9469.2011.00770.x},
  volume       = 39,
  year         = 2012
}

@article{foschigeometry,
  author       = {Introduction, An},
  isbn         = 0821827782,
  title        = {{the Geometry of}}
}

@article{Kr_mer_2007,
  abstract     = {The aim of this paper is twofold. In the first part,
                  we recapitulate the main results regarding the
                  shrinkage properties of partial least squares (PLS)
                  regression. In particular, we give an alternative
                  proof of the shape of the PLS shrinkage factors. It
                  is well known that some of the factors are
                  {\textgreater}1. We discuss in detail the effect of
                  shrinkage factors for the mean squared error of
                  linear estimators and argue that we cannot extend
                  the results to PLS directly, as it is nonlinear. In
                  the second part, we investigate the effect of
                  shrinkage factors empirically. In particular, we
                  point out that experiments on simulated and real
                  world data show that bounding the absolute value of
                  the PLS shrinkage factors by 1 seems to leads to a
                  lower mean squared error.},
  author       = {Kr{\"{a}}mer, Nicole},
  doi          = {10.1007/s00180-007-0038-z},
  issn         = 09434062,
  journal      = {Computational Statistics},
  keywords     = {Biased estimators,Linear regression,Mean squared
                  error},
  month        = {mar},
  number       = 2,
  pages        = {249--273},
  publisher    = {Springer Nature},
  title        = {{An overview on the shrinkage properties of partial
                  least squares regression}},
  url          = {http://dx.doi.org/10.1007/s00180-007-0038-z},
  volume       = 22,
  year         = 2007
}

@article{kramer2012degrees,
  abstract     = {The derivation of statistical properties for Partial
                  Least Squares regression can be a challenging task.
                  The reason is that the construction of latent
                  components from the predictor variables also depends
                  on the response variable. While this typically leads
                  to good performance and interpretable models in
                  practice, it makes the statistical analysis more
                  involved. In this work, we study the intrinsic
                  complexity of Partial Least Squares Regression. Our
                  contribution is an unbiased estimate of its Degrees
                  of Freedom. It is defined as the trace of the first
                  derivative of the fitted values, seen as a function
                  of the response. We establish two equivalent
                  representations that rely on the close connection of
                  Partial Least Squares to matrix decompositions and
                  Krylov subspace techniques. We show that the Degrees
                  of Freedom depend on the collinearity of the
                  predictor variables: The lower the collinearity is,
                  the higher the Degrees of Freedom are. In
                  particular, they are typically higher than the naive
                  approach that defines the Degrees of Freedom as the
                  number of components. Further, we illustrate how our
                  Degrees of Freedom estimate can be used for the
                  comparison of different regression methods. In the
                  experimental section, we show that our Degrees of
                  Freedom estimate in combination with information
                  criteria is useful for model selection.},
  archivePrefix= {arXiv},
  arxivId      = {1002.4112},
  author       = {Kr{\"{a}}mer, Nicole},
  doi          = {10.1198/jasa.2011.tm10107},
  eprint       = {1002.4112},
  issn         = {0162-1459},
  journal      = {Journal of the American Statistical Association},
  keywords     = {Degrees of Freedom,Model selection,Partial Least
                  Squares,Regression},
  number       = 494,
  pages        = {697--705},
  publisher    = {Taylor {\&} Francis},
  title        = {{The Degrees of Freedom of Partial Least Squares
                  Regression}},
  url          = {http://sugiyama-www.cs.titech.ac.},
  volume       = 106,
  year         = 2011
}

@article{Lingjaerde_2000,
  author       = {Lingjaerde, O. C. and Christophersen, Nils},
  doi          = {10.1111/1467-9469.00201},
  issn         = {0303-6898},
  journal      = {Scandinavian Journal of Statistics},
  keywords     = {cient,de,krylov subspace,least squares,partial least
                  squares,principal components,rank,shrinkage
                  estimators,subspace distance},
  month        = {sep},
  number       = 3,
  pages        = {459--473},
  publisher    = {Wiley-Blackwell},
  title        = {{Shrinkage Structure of Partial Least Squares}},
  url          = {http://doi.wiley.com/10.1111/1467-9469.00201},
  volume       = 27,
  year         = 2000
}

@book{martens1992multivariate,
  author       = {{Martens H. and}, Naes T.},
  publisher    = {John Wiley {\&} Sons},
  title        = {{Multivariate Calibration}},
  year         = 1989
}

@article{Martens_2001,
  author       = {Martens, H Martens and M},
  doi          = {10.1088/0957-0233/12/10/708},
  isbn         = {0957-0233},
  issn         = {0957-0233},
  journal      = {Measurement Science and Technology},
  month        = {sep},
  number       = 10,
  pages        = {1746--1746},
  publisher    = {IOP Publishing},
  title        = {{Multivariate Analysis of Quality. An Introduction}},
  url          =
                  {http://stacks.iop.org/0957-0233/12/i=10/a=708?key=crossref.0bf3a4d07d3925303ab1cc8aafab5ef3},
  volume       = 12,
  year         = 2001
}

@article{McCullagh_2002,
  abstract     = {This paper addresses two closely related questions,
                  "What is a statistical model?" and "What is a
                  parameter?" The notions that a model must "make
                  sense," and that a parameter must "have a
                  well-defined meaning" are deeply ingrained in
                  applied statistical work, reasonably well understood
                  at an instinctive level, but absent from most formal
                  theories of modelling and inference. In this paper,
                  these concepts are defined in algebraic terms, using
                  morphisms, functors and natural transformations. It
                  is argued that inference on the basis of a model is
                  not possible unless the model admits a natural
                  extension that includes the domain for which
                  inference is required. For example, prediction
                  requires that the domain include all future units,
                  subjects or time points. Although it is usually not
                  made explicit, every sensible statistical model
                  admits such an extension. Examples are given to show
                  why such an extension is necessary and why a formal
                  theory is required. In the definition of a
                  subparameter, it is shown that certain parameter
                  functions are natural and others are not. Inference
                  is meaningful only for natural parameters. This
                  distinction has important consequences for the
                  construction of prior distributions and also helps
                  to resolve a controversy concerning the Box-Cox
                  model.},
  author       = {McCullagh, Peter},
  doi          = {10.1214/aos/1035844977},
  issn         = 00905364,
  journal      = {Annals of Statistics},
  keywords     = {Aggregation,Agricultural field experiment,Bayes
                  inference,Box-Cox model,Category,Causal
                  inference,Commutative diagram,Conformal
                  model,Contingency
                  table,Embedding,Exchangeability,Extendability,Extensive
                  variable,Fertility effect,Functor,Gibbs model},
  month        = {oct},
  number       = 5,
  pages        = {1225--1267},
  publisher    = {Institute of Mathematical Statistics},
  title        = {{What is a statistical model?}},
  url          = {http://dx.doi.org/10.1214/aos/1035844977},
  volume       = 30,
  year         = 2002
}

@article{mehmood2016diversity,
  author       = {Mehmood, Tahir and Ahmed, Bilal},
  doi          = {10.1002/cem.2762},
  issn         = 08869383,
  journal      = {Journal of Chemometrics},
  keywords     = {10.1002/cem.2762 and
                  PLS,Apllication,Calssification,Regression,chemometrics,classification,computer
                  vision,defense,econometric,environmental
                  studies,genomics,neuroinformatics,partial least
                  squares,process control,regression,variable
                  selection},
  number       = {July},
  pages        = {n/a--n/a},
  publisher    = {Wiley Online Library},
  title        = {{The diversity in the applications of partial least
                  squares: an overview}},
  url          = {http://doi.wiley.com/10.1002/cem.2762},
  volume       = 30,
  year         = 2015
}

@article{munck2010physiochemical,
  abstract     = {An extension of chemometric theory was
                  experimentally explored to explain the
                  physiochemical basis of the very high efficiency of
                  soft modelling of data from nature. Soft modelling
                  in self-organisation was interpreted by studying the
                  unique chemical patterns of mutants in an isogenic
                  barley model on endosperm development. Extremely
                  reproducible, differential Near Infrared (NIR)
                  spectral patterns specifically overviewed the effect
                  on cell composition of each mutant cause. Extended
                  Canonical Variates Analysis (ECVA) classified
                  spectra in wild type, starch and protein mutants.
                  The spectra were interpreted by chemometric data
                  analysis and by pattern inspection to morphological,
                  genetic, molecular and chemical information.
                  Deterministic chemical reactions were defined in the
                  glucan pathway. A drastic mutation in a gene
                  controlling the starch/(ss)-glucan composition
                  changed water activity that introduced a diffusive,
                  stochastic effect on the catalysis of all active
                  enzymes. 'Decision making' in self-organisation is
                  autonomous and performed by the soft modelling of
                  the chemical deterministic and stochastic reactions
                  in the endosperm cell as a whole. Uncertainty in the
                  analysis of endosperm emergence was experimentally
                  delimited as the 'indeterminacy' in local molecular
                  path modelling 'bottom up' and the 'irreducibility'
                  of the phenomenological NIR spectra 'top down'. The
                  experiment confirmed Ilya Prigogine's interpretation
                  of self-organisation by his dynamic computer model
                  programmed with a self-modeled non-local extension
                  of quantum mechanics (QM). The significance of
                  self-organisation explained by Prigogine here
                  interpreted as physiochemical soft modelling
                  introduces a paradigm shift in macroscopic science
                  that forwards a major argument for soft mathematical
                  modelling and chemometrics to obtain full scientific
                  legitimacy. Copyright (C) 2010 John Wiley {\&} Sons,
                  Ltd.},
  author       = {Munck, L. and Jespersen, B. Moller and Rinnan, ??
                  and Seefeldt, H. Fast and Engelsen, M. M??ller and
                  N??rgaard, L. and Engelsen, S. Balling},
  doi          = {10.1002/cem.1278},
  isbn         = {0886-9383},
  issn         = 08869383,
  journal      = {Journal of Chemometrics},
  keywords     = {A unified theory for mathematical interpretation of
                  natural phenomena,Chemical soft modelling as a
                  principle in nature,Near Infrared Spectroscopy
                  (NIRS) endosperm mutant model,Physiochemical
                  analogies to mathematical elements in
                  self-organisation,Scientific legitimacy of soft
                  modelling and chemometrics},
  number       = {7-8},
  pages        = {481--495},
  publisher    = {Wiley Online Library},
  title        = {{A physiochemical theory on the applicability of
                  soft mathematical models-experimentally
                  interpreted}},
  volume       = 24,
  year         = 2010
}

@article{naes1993relevant,
  abstract     = {This paper treats definitions of "relevant
                  components" in regression. A component is defined as
                  a linear combination of the independent variables,
                  and different requirements for such components to be
                  "relevant" for the prediction problem are discussed.
                  The relationship between the definitions is
                  investigated and their relationship to various
                  regression methods is discussed. A new regression
                  method is proposed: restricted principal component
                  regression, which is based on the concept of
                  "relevant components". The theory is illustrated by
                  examples.},
  author       = {N{\ae}s, Tormod and Helland, Inge S},
  isbn         = 0900259450,
  journal      = {Scandinavian Journal of Statistics},
  keywords     = {collinearity,partial least squares
                  regression,prediction,principal component
                  regression,restricted principal component
                  regression},
  number       = 3,
  pages        = {239--250},
  publisher    = {JSTOR},
  title        = {{Relevant components in regression}},
  volume       = 20,
  year         = 1993
}

@article{Naik_2000,
  author       = {Naik, Prasad and Tsai, Chih-Ling},
  doi          = {10.1111/1467-9868.00262},
  isbn         = {1467-9868},
  issn         = {1369-7412},
  journal      = {Journal of the Royal Statistical Society: Series B
                  (Statistical Methodology)},
  keywords     = {collinearity,data reduction,regressor
                  construction,single-index models,sliced},
  month        = {nov},
  number       = 4,
  pages        = {763--771},
  publisher    = {Wiley-Blackwell},
  title        = {{Partial least squares estimator for single-index
                  models}},
  url          = {http://doi.wiley.com/10.1111/1467-9868.00262},
  volume       = 62,
  year         = 2000
}

@article{S√¶b√∏2015,
  abstract     = {In the field of chemometrics and other areas of data
                  analysis the development of new methods for
                  statistical inference and prediction is the focus of
                  many studies. The requirement to document the
                  properties of new methods is inevitable, and often
                  simulated data are used for this purpose. However,
                  when it comes to simulating data there are few
                  standard approaches. In this paper we propose a very
                  transparent and versatile method for simulating
                  response and predictor data from a multiple linear
                  regression model which hopefully may serve as a
                  standard tool simulating linear model data. The
                  approach uses the principle of a relevant subspace
                  for prediction, which is known both from Partial
                  Least Squares and envelope models, and is
                  essentially based on a re-parametrization of the
                  random x regression model. The approach also allows
                  for defining a subset of relevant observable
                  predictor variables spanning the relevant latent
                  subspace, which is handy for exploring methods for
                  variable selection. The data properties are defined
                  by a small set of input-parameters defined by the
                  analyst. The versatile approach can be used to
                  simulate a great variety of data with varying
                  properties in order to compare statistical methods.
                  The method has been implemented in an R-package and
                  its use is illustrated by examples.},
  author       = {S{\ae}b{\o}, Solve and Alm{\o}y, Trygve and Helland,
                  Inge S.},
  doi          = {10.1016/j.chemolab.2015.05.012},
  file         = {:Users/rajurim/Documents/Mendeley
                  Desktop/S{\ae}b{\o}, Alm{\o}y, Helland - 2015 -
                  simrel-A versatile tool for linear model data
                  simulation based on the concept of a relevant
                  subspace an.pdf:pdf},
  issn         = 18733239,
  journal      = {Chemometrics and Intelligent Laboratory Systems},
  keywords     = {Data simulation,Experimental design,Linear
                  model,R-package},
  month        = {aug},
  pages        = {128--135},
  publisher    = {Elsevier},
  title        = {{Simrel - A versatile tool for linear model data
                  simulation based on the concept of a relevant
                  subspace and relevant predictors}},
  volume       = 146,
  year         = 2015
}

@article{Stoica_1998,
  author       = {Stoica, Petre and Soderstrom, Torsten},
  doi          = {10.1111/1467-9469.00085},
  issn         = {0303-6898},
  journal      = {Scandinavian Journal of Statistics},
  month        = {mar},
  number       = 1,
  pages        = {17--24},
  publisher    = {Wiley-Blackwell},
  title        = {{Partial Least Squares: A First-order Analysis}},
  url          = {http://doi.wiley.com/10.1111/1467-9469.00085},
  volume       = 25,
  year         = 1998
}

@article{stone1990continuum,
  abstract     = {The paper addresses the evergreen problem of
                  construction of regressors for use in least squares
                  multiple regression. In the context of a general
                  sequential procedure for doing this, it is shown
                  that, with a particular objective criterion for the
                  construction, the procedures of ordinary least
                  squares and principal components regression occupy
                  the opposite ends of a continuous spectrum, with
                  partial least squares lying in between. There are
                  two adjustable `parameters' controlling the
                  procedure: `alpha', in the continuum [ 0, 1], and
                  `omega', the number of regressors finally accepted.
                  These control parameters are chosen by
                  cross-validation. The method is illustrated by a
                  range of examples of its application.},
  author       = {Stone, M. and Brooks, R. J.},
  doi          = {10.1016/S0198-0254(06)80522-4},
  isbn         = {0035-9246},
  issn         = 01980254,
  journal      = {Royal Statistical Society. Series B},
  number       = 12,
  pages        = 1130,
  publisher    = {JSTOR},
  title        = {{Continuum regression: cross-validated sequentially
                  constructed prediction embracing ordinary least
                  squares, partial least squares and principal
                  components regression}},
  url          =
                  {http://linkinghub.elsevier.com/retrieve/pii/S0198025406805224},
  volume       = 37,
  year         = 1990
}

@article{Sundberg_1999,
  abstract     = {This paper tries first to introduce and motivate the
                  methodology of multivariate calibration. Next a
                  review is given, mostly avoiding technicalities, of
                  the somewhat messy theory of the subject. Two
                  approaches are distinguished: the estimation
                  approach (controlled calibration) and the prediction
                  approach (natural calibration). Among problems
                  discussed are the choice of estimator, the choice of
                  confidence region, methodology for handling
                  situations with more variables than observations,
                  near-collinearities (,vith countermeasures like
                  ridge type regression, principal components
                  regression, partial least squares regression and
                  continuum regression), pretreatment of data, and
                  cross-validation vs true prediction. Examples
                  discussed in detail concern estimation of the age of
                  a rhinoceros from its horn lengths
                  (low-dimensional), and nitrate prediction in
                  waste-water from high-dimensional spectroscopic
                  measurements.},
  author       = {Sundberg, Rolf},
  doi          = {10.1111/1467-9469.00144},
  issn         = {0303-6898},
  journal      = {Scandinavian Journal of Statistics},
  keywords     = {QA Mathematics (inc Computing science)},
  month        = {jun},
  number       = 2,
  pages        = {161--207},
  publisher    = {Wiley-Blackwell},
  title        = {{Multivariate calibration - Direct and indirect
                  regression methodology}},
  url          = {http://kar.kent.ac.uk/17131/},
  volume       = 26,
  year         = 1999
}

@article{wold1984collinearity,
  abstract     = {The use of partial least squares (PLS) for handling
                  collinearities among the independent variables X in
                  multiple regression is discussed. Consecutive
                  estimates (rank 1, 2,...) are obtained using the
                  residuals from previous rank as a new dependent
                  variable y. The PLS method is equivalent to the
                  conjugate gradient method used in Numerical Analysis
                  for related problems. To estimate the "optimal"
                  rank, cross validation is used. Jackknife estimates
                  of the standard errors are thereby obtained with no
                  extra computation. The PLS method is compared with
                  ridge regression and principal components regression
                  on a chemical example of modelling the relation
                  between the measured biological activity and
                  variables describing the chemical structure of a set
                  of substituted phenethylamines. Key},
  archivePrefix= {arXiv},
  arxivId      = {arXiv:1308.0863v1},
  author       = {Wold, S and Ruhe, A and Wold, H and {Dunn III}, W J},
  doi          = {10.1137/0905052},
  eprint       = {arXiv:1308.0863v1},
  isbn         = {0196-5204},
  issn         = {0196-5204},
  journal      = {SIAM J. Sci. and Stat. Comput.},
  keywords     = {collinearity,conjugate gradients,cross
                  validation,linear regression,principal components},
  number       = 3,
  pages        = {735--743},
  publisher    = {SIAM},
  title        = {{The collinearity problem in linear regression. The
                  partial least squares (PLS) approach to generalized
                  inverses}},
  volume       = 5,
  year         = 2013
}
